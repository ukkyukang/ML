{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A1T6blEDejFJ"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDbW229m5/lxZtQPGdGRUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ukkyukang/ML/blob/main/Linear_Regression_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1.1. Ordinary Least Squares\n",
        "\n",
        "LinearRegression 모델을 사용해서 입력한 테스트 데이터를 표현하는 수식을 만들고, Residual의 분산이 최소가 되는 coefficient를 찾아낸다.\n",
        "\n",
        "Ordinary least squares란 입력값 x1을 수식에 넣었은 결과 f(x1)에 대해, 1/n (합 ( x1-f(x1)^2)을 의미한다.\n",
        "\n",
        "\n",
        "## Note:\n",
        "* Mean (평균) → Deviation(편차, 값-평균) → variance(분산, 편차제곱의 합의 평균) → Standard deviation ( 분산에 루트를 씌운 값)\n",
        "* Residual : Deviation와 같은 개념인데, 통계가 아니라 linearRegression 모델에서 생성한 수식에서 예측한 결과 값과 실제 결과 값의 차이. 이 차이가 가장 작은 수식을 만드는 것이 목표\n",
        "\n",
        "## Link\n",
        "* Ordinary_least_squares (OLS) : https://en.wikipedia.org/wiki/\n",
        "* Meas square error (MSE) : https://en.wikipedia.org/wiki/Mean_squared_error\n",
        "regression 함수에서 예측한 값과 실제 값의 차이의 제곱의 평균 (분산같은 개념)\n",
        "* Non-negative least squares : https://en.wikipedia.org/wiki/Non-negative_least_squares\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Mean Squared Error (MSE)**와 **Ordinary Least Squares (OLS)**는 둘 다 회귀 분석에서 사용되는 개념이지만, 그들의 목적과 사용 방식에는 중요한 차이점이 있습니다12345.\n",
        "\n",
        "Mean Squared Error (MSE): MSE는 회귀 모델의 성능을 평가하는 데 사용되는 지표입니다1. MSE는 모델의 예측 값과 실제 값 사이의 차이를 제곱하여 평균을 낸 것입니다1. 이는 모델의 예측 오차를 정량화하는 데 사용되며, 값이 작을수록 모델의 성능이 좋다고 판단합니다. --> **residual/error 개념이 사용되는 것**\n",
        "\n",
        "Ordinary Least Squares (OLS): OLS는 선형 회귀 모델의 매개변수를 추정하는 데 사용되는 방법입니다5. OLS는 모델의 예측 값과 실제 값 사이의 차이의 제곱 합, 즉 MSE를 최소화하는 매개변수를 찾는 것이 목표입니다5. 이는 모델의 매개변수를 학습하는 데 사용되며, 이를 통해 최적의 선형 회귀 모델을 구축합니다. --> **분산이 사용됨**\n",
        "\n",
        "따라서, MSE와 OLS의 주요 차이점은 MSE가 모델의 성능을 평가하는 데 사용되는 반면, OLS는 모델의 매개변수를 학습하는 데 사용된다는 것입니다\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A1T6blEDejFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "reg.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFO9f7lNS3d1",
        "outputId": "cf506a39-04b0-4ebf-af1e-0a432617eef3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Linear Regression Example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)"
      ],
      "metadata": {
        "id": "jv-rsVBJfAbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "# Use only one feature\n",
        "# 모든 행에 대해서, 새로운 column을 만들고, column은 2번 column을 사용하시오\n",
        "# diabetes_X는 pandas DataFrame이 아니라 np.array( 2D array )이다. 아래의 array slicing을\n",
        "# dataframe slicing과 혼동하지 말것.\n",
        "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
        "\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "diabetes_X_train = diabetes_X[:-20]\n",
        "diabetes_X_test = diabetes_X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "diabetes_y_train = diabetes_y[:-20]\n",
        "diabetes_y_test = diabetes_y[-20:]\n",
        "\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(diabetes_X_train, diabetes_y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
        "plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hMKv5HXfCbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.1.1. Non-Negative Least Squares\n",
        "\n",
        "coefficient가 음수가 되지 않도록 설정할 수 있다.\n",
        "\n",
        "Non-Negative Least Squares (NNLS)는 회귀 계수가 음수가 되지 않도록 제한하는 최소제곱 문제의 한 유형입니다1. 이 방법은 다음과 같은 상황에서 유용합니다:\n",
        "\n",
        "변수가 자연적으로 음수가 될 수 없는 상황: 예를 들어, 물리적 크기나 개수 등을 나타내는 변수는 음수가 될 수 없습니다. 이런 경우 NNLS를 사용하면 모델이 물리적으로 불가능한 값을 예측하는 것을 방지할 수 있습니다1.\n",
        "희소성이 필요한 상황: NNLS는 회귀 계수 중 일부를 0으로 만들어 결과를 희소하게 만듭니다2. 이는 변수 선택이 필요한 상황이나 고차원 데이터에서 유용합니다2.\n",
        "행렬 분해: NNLS 문제는 행렬 분해, 예를 들어 PARAFAC와 음수 미포함 행렬/텐서 인수분해 알고리즘에서 하위 문제로 나타납니다1.\n",
        "따라서 NNLS는 이러한 상황에서 유용하게 사용될 수 있습니다. 하지만 NNLS는 모든 상황에 적합한 것은 아니며, 문제의 특성과 데이터에 따라 적절한 모델을 선택해야 합니다."
      ],
      "metadata": {
        "id": "8OSZtSP5gmSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html#sphx-glr-auto-examples-linear-model-plot-nnls-py"
      ],
      "metadata": {
        "id": "39wYlE_xhOo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p(msg,v):\n",
        "\n",
        "  print(msg)\n",
        "  print(v)\n",
        "  print(\"\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples, n_features = 200,50\n",
        "X = np.random.randn(n_samples, n_features) # 무작위 테스트 데이터를 만든다. n_samples : 행 개수, n_features : 열개수\n",
        "p(\"5x4 matrix\",X)\n",
        "\n",
        "# 랜덤 값 4개가 들어있는 배열 생성\n",
        "temp = np.random.randn(n_features)\n",
        "p(\"np.random.randn(n_features):\",temp)\n",
        "\n",
        "# 위에서 생성한 랜던값에 3을 곱한 배열을 생\n",
        "# true_coef = 3 * np.random.randn(n_features)\n",
        "true_coef = 3*temp\n",
        "p(\"true_coef:\",true_coef)\n",
        "\n",
        "true_coef[true_coef < 0] = 0\n",
        "p(\"true_coef[true_coef < 0] = 0\",true_coef)\n",
        "\n",
        "\n",
        "# dot product (why?) - 여기서 만드는 값은 입력 X데이터에 대한 결과값 (라벨)이다.\n",
        "\n",
        "y = np.dot(X, true_coef)\n",
        "p(\"np.dot(X, true_coef)\",y)\n",
        "\n",
        "# Add some noise\n",
        "y += 5 * np.random.normal(size=(n_samples,))\n",
        "p(\"add noise\",y)\n",
        "\n",
        "# split the data in train set and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "\n",
        "p(\"X_train\",X_train)\n",
        "p(\"X_test\",X_test)\n",
        "p(\"y_train\",y_train)\n",
        "p(\"y_test\",y_test)\n",
        "\n",
        "# 모델 트레이닝 - Non-Negative least squres (NNLS)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "reg_nnls = LinearRegression(positive=True)\n",
        "y_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\n",
        "r2_score_nnls = r2_score(y_test, y_pred_nnls)\n",
        "print(\"NNLS R2 score\", r2_score_nnls)\n",
        "\n",
        "# OLS\n",
        "reg_ols = LinearRegression()\n",
        "y_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\n",
        "r2_score_ols = r2_score(y_test, y_pred_ols)\n",
        "print(\"OLS R2 score\", r2_score_ols)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# 두가지 모델에서 찾은 coef_값을 매핑해서 그리기\n",
        "ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n",
        "\n",
        "p(\"reg_ols.coef_\",reg_ols.coef_)\n",
        "p(\"reg_nnls.coef_\",reg_nnls.coef_)\n",
        "\n",
        "# 기준선 그리기 위한 전체 값의 min/max값 얻기\n",
        "low_x, high_x = ax.get_xlim()\n",
        "low_y, high_y = ax.get_ylim()\n",
        "\n",
        "p(\"low_x,high_x\",[low_x,high_x])\n",
        "p(\"low_y,high_y\",[low_y,high_y])\n",
        "\n",
        "low = max(low_x, low_y)\n",
        "high = min(high_x, high_y)\n",
        "\n",
        "# 기준선 그리기\n",
        "ax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\n",
        "ax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\n",
        "ax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")"
      ],
      "metadata": {
        "id": "M0dPsvl9hLFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.1.2. Ordinary Least Squares Complexity\n",
        "\n",
        "\n",
        "* OLS의 복잡성에 대해 말하자면, 이 방법은 X에 대한 singular value decomposition (SVD, 특이값 분해)를 계산하여 최소제곱해를 찾습니다. 만약 X가 (n, p) 크기의 행렬이라면, 이 방법의 복잡성은 : O(np^2+p^3) 입니다.\n",
        "* 이는 n이 p보다 훨씬 클 때, 즉 특징의 수가 데이터 포인트의 수보다 훨씬 작을 때 효과적입니다.\n",
        "* 참고로, OLS는 선형 회귀 문제를 해결하는 가장 일반적인 방법 중 하나이며, 이는 각 데이터 포인트와 회귀선 사이의 수직 거리를 최소화함으로써 최적의 선을 찾습니다.\n",
        "\n",
        "* n은 행의 개수이고, p는 column의 개수라고 할때, column개수가 작을 수록, 효과적이다."
      ],
      "metadata": {
        "id": "rQ928jKAgmrS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Bf8eWndhKbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1.2. Ridge regression and classfication\n",
        "\n",
        "* Ridge (산등성이)\n",
        "\n",
        "Ridge Regression은 선형 회귀 모델에서 과적합을 방지하기 위해 사용되는 정규화 기법입니다. 이 방법은 기본적인 선형 회귀의 비용 함수에 정규화 항을 추가하여 모델의 복잡성을 제어하고, 과적합을 방지합니다12.\n",
        "Ridge Regression의 비용 함수는 다음과 같습니다:\n",
        "RSS+λj=1∑p​βj2​\n",
        "여기서:\n",
        "* RSS는 잔차 제곱합,\n",
        "* λ는 정규화 파라미터,\n",
        "* β는 회귀 계수\n",
        "\n",
        "를 나타냅니다1.\n",
        "* λ가 0일 때, Ridge Regression은 일반적인 최소제곱법과 동일한 결과를 제공합니다.\n",
        "* 그러나 λ가 무한대에 가까워질수록, 정규화 항이 더욱 영향력을 가지게 되어 Ridge Regression의 회귀 계수 추정치는 0에 가까워집니다1\n",
        "\n",
        "* Ridge Regression의 주요 장점은 편향-분산 트레이드오프를 통해 전체 평균 제곱 오차(MSE)를 줄이는 데 있습니다1. 즉, 약간의 편향을 도입하여 분산을 크게 줄이는 것입니다1.\n",
        "\n",
        "* Ridge Classifier는 Ridge Regression의 아이디어를 확장하여 다중 클래스 분류 문제에 적용한 것입니다3. Ridge Classifier는 L2 정규화를 사용하여 과적합을 방지하고, 모델 복잡성과 데이터 적합 사이의 균형을 유지합니다3. 이 분류기의 특징 중 하나는 목표 변수를 -1과 1 사이의 특정 범위로 변환하여 분류 문제를 회귀 프레임워크에 적용하는 것입니다3. 이 변환은 과적합의 가능성을 줄입니다3.\n",
        "Ridge Classifier는 분류와 회귀의 요소를 결합하여 복잡한 분류 문제에 대한 안정적이고 신뢰할 수 있는 해답을 제공합니다3.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ridge Regression과 Lasso Regression은 둘 다 선형 회귀 모델에서 과적합을 방지하기 위한 정규화(regularization) 기법입니다. 이 두 기법의 주요 차이점은 정규화 항의 형태에 있습니다12345.\n",
        "\n",
        "Ridge Regression: Ridge Regression은 L2 정규화를 사용하며, 비용 함수에 계수의 제곱의 합에 대한 패널티 항을 추가합니다1. 이 패널티 항은 계수의 크기를 줄이는 데 도움이 되지만, 계수를 완전히 0으로 만들지는 않습니다1. 이로 인해 Ridge Regression은 모든 특성을 유지하면서도 계수의 크기를 제어합니다1.\n",
        "\n",
        "Lasso Regression: Lasso Regression은 L1 정규화를 사용하며, 비용 함수에 계수의 절대값의 합에 대한 패널티 항을 추가합니다1. 이 패널티 항은 계수의 크기를 줄이는 데 도움이 되며, 일부 계수를 완전히 0으로 만들 수 있습니다1. 이로 인해 Lasso Regression은 특성 선택(feature selection)을 수행하게 되며, 덜 중요한 특성의 계수를 0으로 만들어 모델의 복잡성을 줄입니다1.\n",
        "\n",
        "따라서, Ridge Regression과 Lasso Regression의 주요 차이점은 Ridge Regression이 모든 특성을 유지하면서도 계수의 크기를 제어하는 반면, Lasso Regression은 덜 중요한 특성의 계수를 0으로 만들어 모델의 복잡성을 줄이는 데 있습니다\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "회귀 분석에서 비용 함수는 모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수입니다1. 이 차이는 “오차” 또는 \"비용\"으로 간주되며, 비용 함수의 목표는 이 오차를 최소화하는 것입니다1.\n",
        "선형 회귀에서 가장 일반적으로 사용되는 비용 함수는 평균 제곱 오차(Mean Squared Error, MSE)입니다1. MSE는 모델의 예측 값과 실제 값 사이의 차이를 제곱하여 평균을 낸 것입니다. 수식으로 표현하면 다음과 같습니다:\n",
        "MSE=n1​i=1∑n​(yi​−y^​i​)2\n",
        "여기서 yi​는 실제 값, y^​i​는 모델에 의한 예측 값, n은 데이터 포인트의 개수를 나타냅니다1.\n",
        "비용 함수를 최소화하는 것은 모델의 매개변수(예: 선형 회귀의 기울기와 절편)를 찾는 것을 의미하며, 이 과정은 경사 하강법 등의 최적화 알고리즘을 통해 수행됩니다1.\n"
      ],
      "metadata": {
        "id": "Zpb7F-jtgm_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.2.1. Regression\n",
        "\n",
        "선형회귀 모델에서 과적합(overfitting)은 모델이 학습 데이터에 너무 잘 맞아서 새로운 데이터에는 잘 맞지 않는 현상을 의미합니다123.\n",
        "\n",
        "선형회귀 모델은 독립 변수와 종속 변수 사이의 선형 관계를 모델링하는 방법입니다1. 이 모델은 학습 데이터를 기반으로 독립 변수와 종속 변수 사이의 관계를 학습하며, 이 과정에서 모델이 학습 데이터에 너무 잘 맞게 학습되어 실제 데이터에 대한 예측 성능이 떨어지는 경우가 있습니다123. 이를 과적합이라고 합니다.\n",
        "\n",
        "과적합은 모델이 학습 데이터의 노이즈까지 학습하여, 학습 데이터에만 과도하게 최적화된 상태를 말합니다123. 이런 상태의 모델은 학습 데이터에 대해서는 높은 성능을 보이지만, 새로운 데이터에 대해서는 일반화 성능이 떨어지는 경향이 있습니다123.\n",
        "\n",
        "과적합을 방지하기 위한 방법 중 하나로 Regularization이 있습니다3. Regularization은 회귀 계수에 패널티 값을 적용하여 모델의 복잡도를 제한하고, 이를 통해 과적합을 방지하는 방법입니다3."
      ],
      "metadata": {
        "id": "iR1EZWUgBLBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "reg_1 = linear_model.LinearRegression()\n",
        "reg_2 = linear_model.Ridge(alpha=.5)\n",
        "reg_1.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "reg_2.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "\n",
        "p(\"reg_1.coef_ and intercept_\", [reg_1.coef_, reg_1.intercept_])\n",
        "p(\"reg_2.coef_ and intercept_\", [reg_2.coef_, reg_2.intercept_])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orDu0-2F_V90",
        "outputId": "196331db-ca30-42b1-c61c-c1228e8b55e7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reg_1.coef_ and intercept_\n",
            "[array([0.5, 0.5]), 1.1102230246251565e-16]\n",
            "\n",
            "\n",
            "reg_2.coef_ and intercept_\n",
            "[array([0.44444444, 0.44444444]), 0.11111111111111116]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.2.2. Classification"
      ],
      "metadata": {
        "id": "t_iH6Jx5A0x4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5XJ5ox5Az9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Support"
      ],
      "metadata": {
        "id": "91h1gJd1fRhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.arange(4)\n",
        "print(arr.shape,arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGVGlOg-Jbii",
        "outputId": "60775593-f04c-430d-8ade-65bb93117ee9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,) [0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1D array에, 1개의 row를 추가한다\n",
        "row_vec = arr[np.newaxis, :]\n",
        "print(row_vec.shape,row_vec)\n",
        "\n",
        "col_vec = arr[:,np.newaxis]\n",
        "print(col_vec.shape,col_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc0dBu8mJeT_",
        "outputId": "e408f5ec-50fb-480a-edc8-ff7c0083d4cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4) [[0 1 2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas indexsing basic**"
      ],
      "metadata": {
        "id": "jJLeo6zTYW1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# numpy slicing (Don't confuse with Pandas Dataframe slicing)\n",
        "\n",
        "# 2D array\n",
        "arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
        "print(arr)\n",
        "\n",
        "arr_dataframe = pd.DataFrame(arr, index=[0,1],columns=[\"A\",\"B\",\"C\",\"D\",\"E\"])\n",
        "print(arr_dataframe)\n",
        "\n",
        "# indexing : 0/1행에서 3번째 column의 값\n",
        "print(arr[0:2,2])\n",
        "\n",
        "# slicing : 모든행에 대해, 새로운 axis1(column)을 만들건데, column index는 1\n",
        "print(\"# slicing : 모든행에 대해, 새로운 axis1(column)을 만들건데, column index는 1\")\n",
        "print(arr[:,np.newaxis,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqB0BHuRPjqv",
        "outputId": "2560a896-647f-477e-fac3-247d23fcf79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4  5]\n",
            " [ 6  7  8  9 10]]\n",
            "   A  B  C  D   E\n",
            "0  1  2  3  4   5\n",
            "1  6  7  8  9  10\n",
            "[3 8]\n",
            "# slicing : 모든행에 대해, 새로운 axis1(column)을 만들건데, column index는 1\n",
            "[[2]\n",
            " [7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D Array slicing**"
      ],
      "metadata": {
        "id": "0H-C1O63Yain"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.DataFrame(np.arange(10, 22).reshape(3, 4),\n",
        "                  index=[\"a\", \"b\", \"c\"],\n",
        "                  columns=[\"A\", \"B\", \"C\", \"D\"])\n",
        "\n",
        "# %%\n",
        "# df.loc\n",
        "# 데이터프레임에 index가 설정되어 있을때, 그 index를 참조해서 데이터를 가져오는 것\n",
        "# column 이름만 사용하면 에러가 난다. column을 가져올때에는 먼저 index를 선택해야 한다 [index,column]\n",
        "# df.iloc은 loc와 같은 동작인데, 정수만 받는다.\n",
        "\n",
        "# 행 가져오기, 반환값은 시리즈\n",
        "print('df.loc[\"a\"],\"행 가져오기, 반환값은 시리즈\"')\n",
        "print(df.loc[\"a\"])\n",
        "\n",
        "# 여러행 가져오기 1 : Range\n",
        "print('\\n df.loc[\"b\":\"c\"]','여러행 가져오기 1 : Range (or, df[\"b\":\"c\"])')\n",
        "print(df.loc[\"b\":\"c\"])\n",
        "\n",
        "# 여러행 가져오기-2 : 행 선택\n",
        "print('\\n df.loc[[]\"b\",\"c\"]]','여러행 가져오기-2 : 행 선택 (df.[[]\"b\",\"c\"]] -> loc를 빼면 에러')\n",
        "print(df.loc[\"b\":\"c\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG4bXSg8PZFp",
        "outputId": "d9e23ab3-bf9b-4685-ed22-965a0dae7cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df.loc[\"a\"],\"행 가져오기, 반환값은 시리즈\"\n",
            "A    10\n",
            "B    11\n",
            "C    12\n",
            "D    13\n",
            "Name: a, dtype: int64\n",
            "\n",
            " df.loc[\"b\":\"c\"] 여러행 가져오기 1 : Range (or, df[\"b\":\"c\"])\n",
            "    A   B   C   D\n",
            "b  14  15  16  17\n",
            "c  18  19  20  21\n",
            "\n",
            " df.loc[[]\"b\",\"c\"]] 여러행 가져오기-2 : 행 선택 (df.[[]\"b\",\"c\"]] -> loc를 빼면 에러\n",
            "    A   B   C   D\n",
            "b  14  15  16  17\n",
            "c  18  19  20  21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# np.rand\n",
        "\n",
        "* Providing seed enable code return always same value\n",
        "* np.random.randint(6), np.random.randint(1,20) --> 1개의 정수값 반환\n",
        "* np.random.rand(6), rand(3,2) : 함수 설정 값(범위) 만큼,  0 ~1사이의 균일 분포의 난수 matrix array생성\n",
        "* np.random.randn(6), randn(3,2) : 함수 설정값(범위)만큼, 가우시안 표준 정규 분포에서 난수 matrix array생성\n",
        "* np.random.shuffle : 기존의 데이터의 순서 바꾸기\n",
        "*np.random.choice : 기존의 데이터에서 sampling\n",
        "*np.unique : 데이터에서 중복된 값을 제거하고 중복되지 않은 값의 리스트 출력\n",
        "*np.bincount  : 발생하지 않은 사건에 대해서도 카운트를 해준다."
      ],
      "metadata": {
        "id": "GsHBFl9ehoCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p(msg,v):\n",
        "\n",
        "  print(msg)\n",
        "  print(v)\n",
        "  print(\"\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples, n_features = 5,4\n",
        "X = np.random.randn(n_samples, n_features) # 무작위 테스트 데이터를 만든다. n_samples : 행 개수, n_features : 열개수\n",
        "p(\"5x4 matrix\",X)\n",
        "\n"
      ],
      "metadata": {
        "id": "JRDRN-rAhpP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waqi93Ebhs25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeuKUJwj3Z4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}